---
title: "Superlearner"
output: html_document
date: "2025-05-13"
---

```{r}
df_agg <- df_agg %>%
  mutate(Target = Total_claims/Total_exposure)
```

```{r}
summary(df_agg)
#Make NA's in Avg_claims to 0
df_agg$Avg_claims[is.na(df_agg$Avg_claims)] <- 0
```


```{r}
task <- TaskRegr$new(id = "stack_task", backend = df_agg, target = "Target")

# Define preprocessing pipeline: convert character to factor, then one-hot encode factors
preproc_graph <- ppl("convert_types", 
                     type_from = "character", 
                     type_to = "factor", 
                     affect_columns = selector_type("character")) %>>%
  po("encode", method = "one-hot")

```

```{r}
# Define base learners
lrn_ranger <- lrn("regr.ranger")
lrn_xgb    <- lrn("regr.xgboost")
lrn_lgb    <- lrn("regr.lightgbm")
lrn_lasso  <- lrn("regr.glmnet", alpha = 1)
lrn_ridge  <- lrn("regr.glmnet", alpha = 0)

# Set up hyperparameter search spaces for each learner
param_ranger <- ps(
  mtry            = p_int(lower = 2, upper = floor(sqrt(task$ncol)) ),  # try mtry from 2 up to sqrt of features
  min.node.size   = p_int(lower = 1, upper = 10),
  num.trees       = p_int(lower = 100, upper = 500)
)
param_xgb <- ps(
  nrounds    = p_int(lower = 50, upper = 300),
  max_depth  = p_int(lower = 3, upper = 10),
  eta        = p_dbl(lower = 0.01, upper = 0.3)
)
param_lgb <- ps(
  num_iterations = p_int(lower = 50, upper = 300),
  num_leaves     = p_int(lower = 20, upper = 100),
  learning_rate  = p_dbl(lower = 0.01, upper = 0.3)
)
param_lasso <- ps(
  lambda = p_dbl(lower = -4, upper = 0, trafo = function(x) 10^x)  # tune log10(lambda) from 1e-4 to 1
)
param_ridge <- ps(
  lambda = p_dbl(lower = -4, upper = 0, trafo = function(x) 10^x)
)

# Create AutoTuner wrappers for each base learner
at_ranger <- AutoTuner$new(
  learner = lrn_ranger,
  resampling = rsmp("cv", folds = 2),
  measure = msr("regr.rmse"),
  search_space = param_ranger,
  terminator = trm("evals", n_evals = 20),
  tuner = tnr("random_search")
)
at_xgb <- AutoTuner$new(
  learner = lrn_xgb, resampling = rsmp("cv", folds = 2),
  measure = msr("regr.rmse"), search_space = param_xgb,
  terminator = trm("evals", n_evals = 20), tuner = tnr("random_search")
)

at_lgb <- AutoTuner$new(
  learner = lrn_lgb, resampling = rsmp("cv", folds = 2),
  measure = msr("regr.rmse"), search_space = param_lgb,
  terminator = trm("evals", n_evals = 20), tuner = tnr("random_search")
)

at_lasso <- AutoTuner$new(
  learner = lrn_lasso, resampling = rsmp("cv", folds = 2),
  measure = msr("regr.rmse"), search_space = param_lasso,
  terminator = trm("evals", n_evals = 10), tuner = tnr("random_search")
)

at_ridge <- AutoTuner$new(
  learner = lrn_ridge, resampling = rsmp("cv", folds = 2),
  measure = msr("regr.rmse"), search_space = param_ridge,
  terminator = trm("evals", n_evals = 10), tuner = tnr("random_search")
)

```


```{r}
cv_ranger <- po("learner_cv", learner = at_ranger, id = "ranger")
cv_xgb    <- po("learner_cv", learner = at_xgb, id = "xgb")
cv_lgb    <- po("learner_cv", learner = at_lgb, id = "lgb")
cv_lasso  <- po("learner_cv", learner = at_lasso, id = "lasso")
cv_ridge  <- po("learner_cv", learner = at_ridge, id = "ridge")

# Combine base learner CV pipelines in parallel
base_union <- gunion(list(cv_ranger, cv_xgb, cv_lgb, cv_lasso, cv_ridge))

# Construct the full stacking graph: preprocessing -> base learners CV -> feature union -> meta-learner
stack_graph <- preproc_graph %>>%
  base_union %>>%
  po("featureunion") %>>%
  po("learner", lrn("regr.lm"))

# Wrap the graph into a GraphLearner for training/evaluation
graph_learner <- GraphLearner$new(stack_graph)
```


```{r}
resampling <- rsmp("cv", folds = 5)
rr <- resample(task, graph_learner, resampling, store_models = TRUE)

# Aggregate performance metrics
avg_rmse <- rr$aggregate(msr("regr.rmse"))
avg_rsq  <- rr$aggregate(msr("regr.rsq"))
print(avg_rmse)
print(avg_rsq)
```

