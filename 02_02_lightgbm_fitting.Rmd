---
title: "02_lightgbm_fitting"
output: html_document
---



```{r EDA}
true_split <- readRDS("C:/Code/IntML_Pro1/output/true_split.rds")

reference_date <- as.Date("2015-01-01")

normalize_date_to_days <- function(date_vector) {
  as.numeric(date_vector - reference_date)
}

mv_changed_data <- mv_data %>%
  select(-c(N_claims_year, Date_next_renewal, Date_lapse, Lapse))

date_cols <- mv_changed_data %>%
  select(where(is.Date)) %>%
  colnames()

mv_changed_data <- mv_changed_data %>%
  mutate(across(all_of(date_cols), ~ year(.x), .names = "{.col}_year")) %>%
  mutate(across(all_of(date_cols), ~ month(.x), .names = "{.col}_month")) %>%
  mutate(across(all_of(date_cols), ~ day(.x), .names = "{.col}_day")) %>%
  mutate(across(all_of(date_cols), normalize_date_to_days))

mv_changed_data[] <- lapply(mv_changed_data, function(x) {
  if (is.factor(x) && anyNA(x)) {
    fct_explicit_na(x, na_level = "NA")
  } else {
    x
  }
})

# count of Unique ID in mv_changed_data
mv_changed_data %>%
  summarise(count = n_distinct(ID),
            split_count = length(unique(c(true_split$train_ids, true_split$test_ids))))
```




```{r Make train and test data}
mv_train_data <- mv_changed_data %>%
  filter(ID %in% true_split$train_ids) %>%
  select(-c(ID)) %>%
  as.data.table()

mv_test_data <- mv_changed_data %>%
  filter(ID %in% true_split$test_ids) %>%
  select(-c(ID)) %>%
  as.data.table()

nrow(mv_train_data) + nrow(mv_test_data)
```




```{r}
factors_list <- mv_changed_data %>% select(where(is.factor)) %>% colnames()


mv_train_data_big <- copy(mv_train_data)
mv_test_data_big <- copy(mv_test_data)
for (i in factors_list) {
  for (j in setdiff(factors_list, i)) {
    print(paste("Comparing", i, "and", j))
    temp <- mv_train_data_big %>%
      group_by(!!sym(i), !!sym(j)) %>%
      summarise(mean = mean(Cost_claims_year),
                sd = sd(Cost_claims_year),
                min = min(Cost_claims_year),
                q_95 = quantile(Cost_claims_year, 0.95),
                IQR = IQR(Cost_claims_year),
                max = max(Cost_claims_year)) 
    mv_train_data_big <- left_join(mv_train_data_big, temp, by = c(i, j))
    mv_test_data_big <- left_join(mv_test_data_big, temp, by = c(i, j))
  }
}

```




```{r Learner and task}
learner_lgb_tweedie = lrn("regr.lightgbm", id = "lgb_tweedie", objective = "tweedie", num_iterations = 1000)
learner_featureless = lrn("regr.featureless")


train_task_mv = TaskRegr$new(id = "motor_vehicle_claims_train", backend = mv_train_data, target = "Cost_claims_year")
predict_task_mv = TaskRegr$new(id = "motor_vehicle_claims_test", backend = mv_test_data, target = "Cost_claims_year")

train_task_mv_big = TaskRegr$new(id = "motor_vehicle_claims_train", backend = mv_train_data_big, target = "Cost_claims_year")
predict_task_mv_big = TaskRegr$new(id = "motor_vehicle_claims_test", backend = mv_test_data_big, target = "Cost_claims_year")
```



```{r Set parameter space}
search_space = ps(
  # --- Parameters for External Tuning ---
  learning_rate = p_dbl(lower = 0.005, upper = 0.1, log = TRUE), # Step size shrinkage
  num_leaves = p_int(lower = 10, upper = 512),       # Max leaves in a tree
  max_depth = p_int(lower = 3, upper = 15),          # Max tree depth
  feature_fraction = p_dbl(lower = 0.2, upper = 1.0), # Fraction of features per tree (colsample_bytree)
  bagging_fraction = p_dbl(lower = 0.2, upper = 1.0), # Fraction of data per tree (subsample)
  min_data_in_leaf = p_int(lower = 1, upper = 100) # Min data in a leaf
)

```



```{r}

at = auto_tuner(
  learner = learner_lgb_tweedie,
  resampling = rsmp("cv", folds = 3), # Inner resampling for tuning
  measure = msr("regr.mse"),       # Measure to optimize (RMSE on inner CV folds)
  search_space = search_space,      # Parameter search space (incl. nrounds as regular param)
  tuner = tnr("random_search"),             # Tuning algorithm
  terminator = trm("evals", n_evals = 10)           # Stopping condition for the external tuning
)
at_big = auto_tuner(
  learner = learner_lgb_tweedie,
  resampling = rsmp("cv", folds = 3), # Inner resampling for tuning
  measure = msr("regr.mse"),       # Measure to optimize (RMSE on inner CV folds)
  search_space = search_space,      # Parameter search space (incl. nrounds as regular param)
  tuner = tnr("random_search"),             # Tuning algorithm
  terminator = trm("evals", n_evals = 10)           # Stopping condition for the external tuning
)
```

```{r}
# --- 6. Train the Model with Tuning ---
at$train(train_task_mv)
at_big$train(train_task_mv_big)


at_lgb_met_tweedie$train(train_task_mv)
```



```{r}
learner_featureless$train(train_task_mv)
print("Featureless")
learner_featureless$predict(predict_task_mv)$score()
# print("LightGBM")
# at$predict(predict_task_mv)$score()
print("Big LightGBM")
at_lgb_met_tweedie$predict(predict_task_mv)$score()
```

```{r}
# Get the best parameters from at_lgb_met_tweedie
at_lgb_met_tweedie$learner$param_set$values
```



```{r}
at_big$predict(predict_task_mv_big) %>%
  as.data.table()


at$predict(predict_task_mv) %>%
  as.data.table() %>%
  arrange(desc(truth)) %>%
  mutate(response = round(response, 2),
         diff = truth - response)

```



```{r}
saveRDS(at, 'lightgbm_rpart.rds')
model <- readRDS('lightgbm_rpart.rds')

model$model$tuning_instance
model$predict(task_mv, row_ids = split$test)$score()

```




