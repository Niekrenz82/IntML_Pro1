---
title: "02_04_classifmodel"
output: html_document
date: "2025-05-12"
---

```{r}
mv_changed_data <- df_agg %>%
  select(-c(ID,Total_N_claims_year)) 

date_cols <- mv_changed_data %>%
  select(where(is.Date)) %>%
  colnames()

mv_changed_data <- mv_changed_data %>%
    mutate(across(all_of(date_cols), ~ year(.x), .names = "{.col}_year")) %>%
    mutate(across(all_of(date_cols), ~ month(.x), .names = "{.col}_month")) %>%
    mutate(across(all_of(date_cols), ~ day(.x), .names = "{.col}_day")) %>%
  mutate(across(all_of(date_cols), normalize_date_to_days))
```

```{r}
str(df_agg)
```



```{r}
df_model <- mv_changed_data %>% select(-Total_claims, -N_claims_history, -R_Claims_history,
                                       -Freq, -Avg_claims)

# Opret task
task_rf = TaskClassif$new(id = "pcc_rf", backend = df_model, target = "PCC", positive = "1")

# Vælg Random Forest learner
learner_rf = lrn("classif.ranger", predict_type = "prob")

# Opret en pipeline bestående af preprocessing og læreren
learner_rf = lrn("classif.ranger", predict_type = "prob")
pipeline_rf = po("encode", method = "one-hot") %>>% learner_rf

# Krydsvalidering (5-fold)
resampling = rsmp("cv", folds = 5)

# Træn og evaluer modellen
rr_rf = resample(task_rf, pipeline_rf, resampling)

# Print performance (f.eks. AUC)
rr_rf$aggregate(list(
  msr("classif.auc"),
  msr("classif.acc"),
  msr("classif.bbrier")
))
```

```{r}
# Opret task
task_xgb = TaskClassif$new(id = "pcc_xgboost", backend = df_model, target = "PCC", positive = "1")

# Opret en pipeline bestående af preprocessing og læreren
learner_xgb = lrn("classif.xgboost", predict_type = "prob")
pipeline_xgb = po("encode", method = "one-hot") %>>% learner_xgb

# Træn og evaluer modellen
rr_xgb = resample(task_xgb, pipeline_xgb, resampling)

# Print performance (f.eks. AUC)
rr_xgb$aggregate(list(
  msr("classif.auc"),
  msr("classif.acc"),
  msr("classif.bbrier")
))
```

```{r, warning=TRUE}
task_lr = TaskClassif$new(id = "pcc_logreg", backend = df_model, target = "PCC", positive = "1")

# Opret pipeline med one-hot encoding + logistisk regression
pipeline_lr = po("encode", method = "one-hot") %>>% lrn("classif.glmnet", predict_type = "prob")

# Træn og evaluer modellen
rr_lr = resample(task_lr, pipeline_lr, resampling)

# Print performance (f.eks. AUC)
rr_lr$aggregate(list(
  msr("classif.auc"),
  msr("classif.acc"),
  msr("classif.bbrier")
))
```


```{r}
task_pcc = TaskClassif$new(id = "pcc_est", backend = df_model, target = "PCC", positive = "1")

# Set a column as the weight column
task_pcc$set_col_role(cols = "total_exposure", new_roles = "weight")

# Define your encoding pipe
pipe_encode = po("encode",
  method         = "treatment",
  affect_columns = selector_type("factor")
)

# Helper to wrap any learner in your pipe
make_graph_learner = function(learner) {
  gl = pipe_encode %>>% learner
  GraphLearner$new(gl)
}

resampling_inner   = rsmp("cv", folds = 2)
measure_auc        = msr("classif.auc")
tuner_rs           = tnr("random_search")
terminator_evals = trm("evals", n_evals = 25)
terminator_evals_small = trm("evals", n_evals = 5)

base_ranger_pcc = lts(lrn("classif.ranger"))
gl_ranger_pcc   = make_graph_learner(base_ranger_pcc)
at_ranger_pcc   = AutoTuner$new(
  learner      = gl_ranger_pcc,
  resampling   = resampling_inner,
  measure      = measure_mse,
  tuner        = tuner_rs,
  terminator   = terminator_evals_small
)

base_xgb_pcc = lts(lrn("classif.xgboost"))
gl_xgb_pcc   = make_graph_learner(base_xgb_pcc)
at_xgb_pcc   = AutoTuner$new(
  learner      = gl_xgb_pcc,
  resampling   = resampling_inner,
  measure      = measure_auc,
  tuner        = tuner_rs,
  terminator   = terminator_evals_small
)

base_lgbm_pcc = mlr3extralearners::lrn("classif.lightgbm",
  learning_rate_pcc = to_tune(0.005, 0.1, logscale = TRUE),
  num_leaves_pcc = to_tune(10, 512),
  max_depth_pcc = to_tune(3, 15),
  feature_fraction_pcc = to_tune(0.6, 1.0),
  bagging_fraction_pcc = to_tune(0.6, 1.0))
gl_lgbm_pcc   = make_graph_learner(base_lgbm_pcc)
at_lgbm_pcc   = AutoTuner$new(
  learner      = gl_lgbm_pcc,
  resampling   = resampling_inner,
  measure      = measure_auc,
  tuner        = tuner_rs,
  terminator   = terminator_evals        # pulls the xgboost‐space, namespaced
)

```



```{r}
learner$train(task)
pred = learner$predict(task)
```

