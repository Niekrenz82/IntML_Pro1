---
title: "02_04_classifmodel"
output: html_document
date: "2025-05-12"
---

```{r}
df_agg <- df_agg %>%
  mutate(PCC = as.factor(ifelse(Total_claims > 0, "1", "0")))
```

```{r}
df_agg_u_dates <- df_agg %>%
  select(-c(Date_start_contract,Date_last_renewal))
```


```{r}
normalize_date_to_days <- function(date_vector) {
  as.numeric(date_vector - as.Date("2015-01-01"))
}

mv_changed_data <- df_agg %>%
  select(-c(ID,N_claims_year)) 

date_cols <- mv_changed_data %>%
  select(where(is.Date)) %>%
  colnames()

mv_changed_data <- mv_changed_data %>%
    mutate(across(all_of(date_cols), ~ year(.x), .names = "{.col}_year")) %>%
    mutate(across(all_of(date_cols), ~ month(.x), .names = "{.col}_month")) %>%
    mutate(across(all_of(date_cols), ~ day(.x), .names = "{.col}_day")) %>%
  mutate(across(all_of(date_cols), normalize_date_to_days))
```

```{r}
str(df_agg)
```



```{r}
df_model <- mv_changed_data %>% select(-Total_claims, -N_claims_history, -R_Claims_history)

# Opret task
task_rf = TaskClassif$new(id = "pcc_rf", backend = df_model, target = "PCC", positive = "1")

# Vælg Random Forest learner
learner_rf = lrn("classif.ranger", predict_type = "prob")

# Opret en pipeline bestående af preprocessing og læreren
learner_rf = lrn("classif.ranger", predict_type = "prob")
pipeline_rf = po("encode", method = "one-hot") %>>% learner_rf

# Krydsvalidering (5-fold)
resampling = rsmp("cv", folds = 5)

# Træn og evaluer modellen
rr_rf = resample(task_rf, pipeline_rf, resampling)

# Print performance (f.eks. AUC)
rr_rf$aggregate(list(
  msr("classif.auc"),
  msr("classif.acc"),
  msr("classif.bbrier")
))
```

```{r}
# Opret task
task_xgb = TaskClassif$new(id = "pcc_xgboost", backend = df_model, target = "PCC", positive = "1")

# Opret en pipeline bestående af preprocessing og læreren
learner_xgb = lrn("classif.xgboost", predict_type = "prob")
pipeline_xgb = po("encode", method = "one-hot") %>>% learner_xgb

# Træn og evaluer modellen
rr_xgb = resample(task_xgb, pipeline_xgb, resampling)

# Print performance (f.eks. AUC)
rr_xgb$aggregate(list(
  msr("classif.auc"),
  msr("classif.acc"),
  msr("classif.bbrier")
))
```

```{r, warning=TRUE}
task_lr = TaskClassif$new(id = "pcc_logreg", backend = df_model, target = "PCC", positive = "1")

# Opret pipeline med one-hot encoding + logistisk regression
pipeline_lr = po("encode", method = "one-hot") %>>% lrn("classif.glmnet", predict_type = "prob")

# Træn og evaluer modellen
rr_lr = resample(task_lr, pipeline_lr, resampling)

# Print performance (f.eks. AUC)
rr_lr$aggregate(list(
  msr("classif.auc"),
  msr("classif.acc"),
  msr("classif.bbrier")
))
```


```{r}
task_pcc = TaskClassif$new(id = "pcc_est", backend = df_model, target = "PCC", positive = "1")

# Define your encoding pipe
pipe_encode = po("encode",
  method         = "treatment",
  affect_columns = selector_type("factor")
)

# Helper to wrap any learner in your pipe
make_graph_learner = function(learner) {
  gl = pipe_encode %>>% learner
  GraphLearner$new(gl)
}

resampling_inner   = rsmp("cv", folds = 2)
measure_auc        = msr("classif.auc")
tuner_rs           = tnr("random_search")
terminator_evals = trm("evals", n_evals = 25)
terminator_evals_small = trm("evals", n_evals = 5)

base_ranger = lts(lrn("classif.ranger"))
gl_ranger   = make_graph_learner(base_ranger)
at_ranger   = AutoTuner$new(
  learner      = gl_ranger,
  resampling   = resampling_inner,
  measure      = measure_mse,
  tuner        = tuner_rs,
  terminator   = terminator_evals_small
)


```



```{r}
learner$train(task)
pred = learner$predict(task)
```

