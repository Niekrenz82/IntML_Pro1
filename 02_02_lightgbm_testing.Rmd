---
title: "02_xgboost_fitting"
output: html_document
date: "2025-05-07"
---



```{r}
reference_date <- as.Date("2015-01-01")

normalize_date_to_days <- function(date_vector) {
  as.numeric(date_vector - reference_date)
}

mv_changed_data <- mv_data %>%
  select(-c(N_claims_year, Date_next_renewal, Date_lapse, Lapse, ID))

date_cols <- mv_changed_data %>%
  select(where(is.Date)) %>%
  colnames()

mv_changed_data <- mv_changed_data %>%
  mutate(across(all_of(date_cols), ~ year(.x), .names = "{.col}_year")) %>%
  mutate(across(all_of(date_cols), ~ month(.x), .names = "{.col}_month")) %>%
  mutate(across(all_of(date_cols), ~ day(.x), .names = "{.col}_day")) %>%
  mutate(across(all_of(date_cols), normalize_date_to_days))

```





```{r Task and learners}
set.seed(1234)
task_mv = TaskRegr$new(id = "motor_vehicle_claims", backend = mv_changed_data, target = "Cost_claims_year")
splits <- partition(task_mv, ratio = 0.8)
```





```{r Task and learners}
# --- 3. Learner Definitions ---

# Featureless learner (baseline)
learner_featureless = lrn("regr.featureless")

# Base LightGBM learners (objective and iterations are set, other params will be tuned)
# Note: `num_iterations` is fixed here. If you want to tune it, add it to the `search_space`.
# The default objective for regr.lightgbm is "regression_l2" (L2 loss)
learner_lgb_base = lrn("regr.lightgbm", id = "lgb_l2", num_iterations = 1000)
learner_lgb_poisson_base = lrn("regr.lightgbm", id = "lgb_poisson", objective = "poisson", num_iterations = 1000)
learner_lgb_tweedie_base = lrn("regr.lightgbm", id = "lgb_tweedie", objective = "tweedie", num_iterations = 1000)

learner_lgb_tweedie_met = lrn("regr.lightgbm", id = "lgb_tweedie_met", objective = "regression", num_iterations = 1000, eval = "tweedie")
learner_lgb_met_tweedie = lrn("regr.lightgbm", id = "lgb_met_tweedie", objective = "tweedie", num_iterations = 1000, eval = "regression")
learner_lgb_quantile_base = lrn("regr.lightgbm", id = "lgb_quantile", objective = "quantile", num_iterations = 1000)

learner_lgb_gamma_base = lrn("regr.lightgbm", id = "lgb_gamma", objective = "gamma", num_iterations = 1000)


# --- 4. Hyperparameter Search Space for LightGBM ---
search_space_lgb = ps(
  learning_rate = p_dbl(lower = 0.005, upper = 0.1, log = TRUE),
  num_leaves = p_int(lower = 10, upper = 300), # Adjusted upper limit from 512 for potentially faster tuning
  max_depth = p_int(lower = 3, upper = 12),     # Adjusted upper limit from 15
  feature_fraction = p_dbl(lower = 0.6, upper = 1.0),
  bagging_fraction = p_dbl(lower = 0.6, upper = 1.0),
  min_data_in_leaf = p_int(lower = 1, upper = 50) # Adjusted upper limit from 100
  # lambda_l1 = p_dbl(lower = 1e-8, upper = 10.0, log = TRUE), # Example: L1 regularization
  # lambda_l2 = p_dbl(lower = 1e-8, upper = 10.0, log = TRUE)  # Example: L2 regularization
  # Consider adding regularization terms if overfitting is an issue.
)

# --- 5. Nested Cross-Validation Setup ---

# Inner resampling (for hyperparameter tuning)
inner_resampling = rsmp("cv", folds = 3)

# Terminator (for hyperparameter tuning)
# Increase n_evals for more thorough tuning in a real scenario
terminator = trm("evals", n_evals = 30) # Using 10 evals for speed; consider 20-50+

# Tuner (Random Search)
tuner = tnr("random_search")

# AutoTuner for each LightGBM variant
# Uses Mean Squared Error (regr.mse) for tuning by default for regression tasks

at_lgb_l2 = AutoTuner$new(
  learner = learner_lgb_base,
  resampling = inner_resampling,
  measure = msr("regr.mse"),
  search_space = search_space_lgb,
  terminator = terminator,
  tuner = tuner
)

at_lgb_poisson = AutoTuner$new(
  learner = learner_lgb_poisson_base,
  resampling = inner_resampling,
  measure = msr("regr.mse"),
  search_space = search_space_lgb,
  terminator = terminator,
  tuner = tuner
)

at_lgb_tweedie = AutoTuner$new(
  learner = learner_lgb_tweedie_base,
  resampling = inner_resampling,
  measure = msr("regr.mse"),
  search_space = search_space_lgb,
  terminator = terminator,
  tuner = tuner
)

# learner_lgb_tweedie_met, learner_lgb_met_tweedie, learner_lgb_quantile_base
at_lgb_tweedie_met = AutoTuner$new(
  learner = learner_lgb_tweedie_met,
  resampling = inner_resampling,
  measure = msr("regr.mse"),
  search_space = search_space_lgb,
  terminator = terminator,
  tuner = tuner
)
at_lgb_met_tweedie = AutoTuner$new(
  learner = learner_lgb_met_tweedie,
  resampling = inner_resampling,
  measure = msr("regr.mse"),
  search_space = search_space_lgb,
  terminator = terminator,
  tuner = tuner
)
at_lgb_quantile = AutoTuner$new(
  learner = learner_lgb_quantile_base,
  resampling = inner_resampling,
  measure = msr("regr.mse"),
  search_space = search_space_lgb,
  terminator = terminator,
  tuner = tuner
)



# List of learners for the outer loop benchmark
# learners_for_benchmark = list(
#   at_lgb_l2,
#   at_lgb_poisson,
#   at_lgb_tweedie,
#   learner_featureless # Baseline
# )

learners_for_benchmark = list(
  at_lgb_tweedie_met,
  at_lgb_met_tweedie,
  at_lgb_quantile,
  learner_featureless # Baseline
)

# Outer resampling (for model evaluation)
outer_resampling = rsmp("cv", folds = 3) # This was your cv10, now explicitly 5 folds
```


```{r}
# --- 6. Benchmarking ---
# Define the design for the benchmark
design = benchmark_grid(
  tasks = task_mv,
  learners = learners_for_benchmark,
  resamplings = outer_resampling
)

# Run the benchmark
# This will take some time due to nested resampling (tuning within each outer fold)
# Consider logger_list = list(lgr::get_logger("mlr3")) and lgr::threshold("info") or "debug" for more verbose output
#future::plan("multisession") # Uncomment for parallel execution if your machine has multiple cores

bmr = benchmark(design)
```

```{r}
bmr$aggregate(msrs(c("regr.mse", "regr.mae")))
$param_set$values

```

```{r}
# --- 7. Finding parameters for best model
# Split data as well

splits <- partition(task_mv, ratio = 0.6)

at_lgb_tweedie$train(
  task = task_mv,
  row_ids = splits$train
)

```

